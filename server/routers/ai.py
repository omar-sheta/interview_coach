"""
AI services router.

Endpoints for LLM generation and question editing.
(TTS/STT services archived - using written-only interaction now)
"""

import logging
import requests
from datetime import datetime
from fastapi import APIRouter, HTTPException, Response

from ..config import settings
from ..models.schemas import GenerateRequest, EditQuestionRequest
from ..services.question_service import build_questions_payload
from ..utils.helpers import check_ollama_available

router = APIRouter()
logger = logging.getLogger("guided_learning.ai_router")

# TTS/STT archived - marked as unavailable
stt_available = False
tts_available = False


@router.get("/health")
async def health_check():
    """Server health status."""
    llm_status = {
        "llm": "unknown",
        "llm_reason": None
    }
    ok, reason = check_ollama_available(
        settings.OLLAMA_BASE_URL,
        timeout=settings.OLLAMA_PROBE_TIMEOUT,
        retries=settings.OLLAMA_PROBE_RETRIES,
        backoff=settings.OLLAMA_PROBE_BACKOFF
    )
    llm_status["llm"] = "available" if ok else "unavailable"
    llm_status["llm_reason"] = None if ok else reason

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "stt": "available" if stt_available else "unavailable",
            "tts": "available" if tts_available else "unavailable",
        },
        **llm_status,
    }


# TTS Endpoint - Archived (returns 503)
@router.post("/synthesize")
async def synthesize_speech():
    """TTS service archived - using written-only interaction now."""
    raise HTTPException(
        status_code=503, 
        detail="TTS service archived. Platform now uses written-only interaction."
    )


@router.post("/generate")
async def generate_text(request: GenerateRequest):
    """Generate interview-ready questions, falling back to templates if needed."""
    try:
        payload = build_questions_payload(request)
        return payload
    except ValueError as ve:
        raise HTTPException(status_code=422, detail=str(ve))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Text generation failed: {str(e)}")


@router.post("/questions/edit")
async def edit_question_with_ai(request: EditQuestionRequest):
    """Edit an existing interview question using an AI instruction."""
    try:
        if not request.original_question or not request.edit_instruction:
            raise HTTPException(status_code=422, detail="Original question and edit instruction are required.")

        context = ""
        if request.job_role:
            context += f"The job role is: {request.job_role}. "
        if request.job_description:
            context += f"The job description is: {request.job_description}. "

        prompt = (
            f"{context}You are an expert at refining interview questions. "
            f"Your task is to edit the following interview question based on the instruction provided. "
            f"Return only the single, edited question, without any preamble or explanation.\n\n"
            f"Original Question: \"{request.original_question}\"\n"
            f"Instruction: \"{request.edit_instruction}\"\n\n"
            f"Edited Question:"
        )

        messages = [{"role": "user", "content": prompt}]

        ollama_response = requests.post(
            f"{settings.OLLAMA_BASE_URL}/api/chat",
            json={
                "model": request.model,
                "messages": messages,
                "stream": False,
                "options": {"temperature": request.temperature, "num_predict": 200},
            },
            timeout=60,
        )

        if ollama_response.status_code == 200:
            result = ollama_response.json()
            content = result.get("message", {}).get("content", "").strip()
            
            # Clean the response to be just the question
            edited_question = content.split('\n')[0].strip()
            if edited_question.startswith('"') and edited_question.endswith('"'):
                edited_question = edited_question[1:-1]

            if not edited_question:
                raise ValueError("LLM returned an empty response.")

            return {
                "edited_question": edited_question,
                "original_question": request.original_question,
                "model": request.model,
                "timestamp": datetime.now().isoformat(),
            }

        detail = ollama_response.text or "LLM editing failed"
        raise RuntimeError(f"LLM editing failed ({ollama_response.status_code}): {detail}")

    except Exception as e:
        logger.error(f"Failed to edit question with AI: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to edit question: {str(e)}")
